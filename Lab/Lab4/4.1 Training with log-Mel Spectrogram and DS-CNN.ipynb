{"cells":[{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693925842804,"execution_millis":2471,"deepnote_app_coordinates":{"h":11,"w":12,"x":0,"y":1},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":false,"deepnote_app_is_output_hidden":true,"cell_id":"2b3e265f12104571913cbc7335574400","deepnote_cell_type":"code"},"source":"import tensorflow as tf","block_group":"c8c264f3df3346cd8cfe69048a4491cf","execution_count":null,"outputs":[{"name":"stderr","text":"2023-09-05 14:57:22.974175: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-09-05 14:57:23.036500: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-09-05 14:57:23.037851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-09-05 14:57:24.027340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/d5ed55cf-cbb1-4781-a1e1-5178c898ea95","content_dependencies":null},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"7c739b6307074f82930e540b6e70adce","deepnote_cell_type":"markdown"},"source":"# Define Hyper-Parameters","block_group":"4d44f2497f374ece9389fc86baec77a3"},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693925845282,"execution_millis":17,"deepnote_app_coordinates":{"h":11,"w":12,"x":0,"y":29},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_output_hidden":true,"cell_id":"f3a4b1ccfb4d4d578de3118d016ccd16","deepnote_cell_type":"code"},"source":"PREPROCESSING_ARGS = {\n    'sampling_rate': 16000,\n    'frame_length_in_s': 0.04,\n    'frame_step_in_s': 0.02,\n    'num_mel_bins': 40,\n    'lower_frequency': 20,\n    'upper_frequency': 4000,\n}\n\nTRAINING_ARGS = {\n    'batch_size': 20,\n    'initial_learning_rate': 0.01,\n    'end_learning_rate': 1.e-5,\n    'epochs': 10\n}","block_group":"d0e034852d654a5ba68fb2cbc98c99f4","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"tags":[],"deepnote_app_coordinates":{"h":4,"w":12,"x":0,"y":41},"deepnote_app_block_visible":true,"cell_id":"63af6c9891164af197f5b5c6064a7bd3","deepnote_cell_type":"markdown"},"source":"# Create Train/Val/Test Datasets","block_group":"89a572f8b0ff430cbdf4baef0d203454"},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693925845305,"execution_millis":446,"deepnote_app_coordinates":{"h":27,"w":12,"x":0,"y":46},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_output_hidden":true,"cell_id":"bbb34491ce674219858cf130cc8465a3","deepnote_cell_type":"code"},"source":"train_ds = tf.data.Dataset.list_files('msc-train/*')\nval_ds = tf.data.Dataset.list_files('msc-val/*')\ntest_ds = tf.data.Dataset.list_files('msc-test/*')","block_group":"0b28a527fb7a427ea592b3f38cec9c09","execution_count":null,"outputs":[{"name":"stderr","text":"2022-11-11 14:42:11.267884: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-11-11 14:42:11.267927: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2022-11-11 14:42:11.267949: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-bdbc19a1-4d38-4250-8aec-b2a48a575b76): /proc/driver/nvidia/version does not exist\n2022-11-11 14:42:11.268375: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-11 14:42:11.806144: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n2022-11-11 14:42:11.806380: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 AVX512F FMA\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-11-11 14:42:12.251692: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-11-11 14:42:12.253269: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-11-11 14:42:12.253457: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","output_type":"stream"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"TensorShape([49, 321])"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/f190bc95-8d25-4b7e-badd-518cc7ddb59f","content_dependencies":null},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693925845737,"execution_millis":995,"deepnote_app_coordinates":{"h":24,"w":12,"x":0,"y":74},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_output_hidden":true,"cell_id":"7a964013e7cd4d26a9b53fc9e77dd514","deepnote_cell_type":"code"},"source":"from preprocessing import LABELS\nfrom preprocessing import AudioReader\nfrom preprocessing import MelSpectrogram\n\n\naudio_reader = AudioReader(tf.int16, 16000)\nmel_spec_processor = MelSpectrogram(**PREPROCESSING_ARGS)\n\ndef prepare_for_training(feature, label):\n    feature = tf.expand_dims(feature, -1)\n    label_id = tf.argmax(label == LABELS)\n\n    return feature, label_id\n\n\nbatch_size = TRAINING_ARGS['batch_size']\nepochs = TRAINING_ARGS['epochs']\n\ntrain_ds = (train_ds\n            .map(audio_reader.get_audio_and_label)\n            .map(mel_spec_processor.get_mel_spec_and_label)\n            .map(prepare_for_training)\n            .batch(batch_size)\n            .cache())\nval_ds = (val_ds\n            .map(audio_reader.get_audio_and_label)\n            .map(mel_spec_processor.get_mel_spec_and_label)\n            .map(prepare_for_training)\n            .batch(batch_size))\ntest_ds = (test_ds\n            .map(audio_reader.get_audio_and_label)\n            .map(mel_spec_processor.get_mel_spec_and_label)\n            .map(prepare_for_training)\n            .batch(batch_size))","block_group":"7464651497924af4becf2c3c4dc6f2bc","execution_count":null,"outputs":[{"name":"stderr","text":"2023-09-05 14:57:25.908834: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n2023-09-05 14:57:25.908988: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX AVX2 AVX512F FMA\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/b53d799b-109e-4947-9bbb-e3b6ee4bb728","content_dependencies":null},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693925846734,"execution_millis":472,"deepnote_app_coordinates":{"h":9,"w":12,"x":0,"y":99},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_output_hidden":true,"cell_id":"f75a9f04fc974feb8f9cd37fdd68efda","deepnote_cell_type":"code"},"source":"for example_batch, example_labels in train_ds.take(1):\n  print('Batch Shape:', example_batch.shape)\n  print('Data Shape:', example_batch.shape[1:])\n  print('Labels:', example_labels)","block_group":"e86f9333ec7548bbbb7efd8a2810d676","execution_count":null,"outputs":[{"name":"stdout","text":"Batch Shape: (20, 49, 40, 1)\nData Shape: (49, 40, 1)\nLabels: tf.Tensor([1 0 3 1 4 4 0 1 3 3 1 6 1 2 0 7 2 2 4 3], shape=(20,), dtype=int64)\n2023-09-05 14:57:27.166214: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/90e46504-fb4f-4cfa-a711-b3c76c285daf","content_dependencies":null},{"cell_type":"markdown","metadata":{"tags":[],"deepnote_app_coordinates":{"h":4,"w":12,"x":0,"y":123},"deepnote_app_block_visible":true,"cell_id":"74c60f71a2884e648a49c2786d46a3ce","deepnote_cell_type":"markdown"},"source":"# Create the Model (DS-CNN)","block_group":"e211c0355b5a4c0b8d2f307626100996"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"7e700a6865dd4f2ebc4e69f8ebfdc9b9","deepnote_cell_type":"markdown"},"source":"DS-CNN\nConv2D(filters=256, kernel_size=[3, 3], stride=[2, 2],                   \n       use_bias=False, padding=’valid’)\nBatchNormalization()\nReLU()\nDepthwiseConv2D(kernel_size=[3, 3], stride=[1, 1], \n                use_bias=False, padding=’same’)\nConv2D(filters=256, kernel_size=[1, 1], stride=[1, 1],   \n       use_bias=False)\nBatchNormalization()\nReLU()\nDepthwiseConv2D(kernel_size=[3, 3], stride=[1, 1],\n                use_bias=False, padding=’same’)\nConv2D(filters=256, kernel_size=[1, 1], stride=[1, 1],   \n       use_bias=False)\nBatchNormalization()\nReLU()\nGlobalAveragePooling2D()\nDense(units=8)\nSoftmax()\n","block_group":"9d20c290b3c443e5866cf71d139c7584"},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693925847197,"execution_millis":171,"deepnote_app_coordinates":{"h":13,"w":12,"x":0,"y":109},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_output_hidden":true,"cell_id":"a529a204df1b4161831cc498f789769c","deepnote_cell_type":"code"},"source":"model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n    tf.keras.layers.Conv2D(filters=256, kernel_size=[3, 3], strides=[2, 2],\n        use_bias=False, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=256, kernel_size=[1, 1], strides=[1, 1],   \n       use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1],\n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=256, kernel_size=[1, 1], strides=[1, 1],   \n       use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(units=len(LABELS)),\n    tf.keras.layers.Softmax()\n])","block_group":"895c002b30ce49ad90c0161e27b2751b","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693925847367,"execution_millis":477,"deepnote_app_coordinates":{"h":27,"w":12,"x":0,"y":141},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_output_hidden":true,"cell_id":"ad9b79a9190e4b298231e3547101d49e","deepnote_cell_type":"code"},"source":"model.summary()","block_group":"54ba371e9ca74036be1d0ec398155e0b","execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 24, 19, 256)       2304      \n                                                                 \n batch_normalization (Batch  (None, 24, 19, 256)       1024      \n Normalization)                                                  \n                                                                 \n re_lu (ReLU)                (None, 24, 19, 256)       0         \n                                                                 \n depthwise_conv2d (Depthwis  (None, 24, 19, 256)       2304      \n eConv2D)                                                        \n                                                                 \n conv2d_1 (Conv2D)           (None, 24, 19, 256)       65536     \n                                                                 \n batch_normalization_1 (Bat  (None, 24, 19, 256)       1024      \n chNormalization)                                                \n                                                                 \n re_lu_1 (ReLU)              (None, 24, 19, 256)       0         \n                                                                 \n depthwise_conv2d_1 (Depthw  (None, 24, 19, 256)       2304      \n iseConv2D)                                                      \n                                                                 \n conv2d_2 (Conv2D)           (None, 24, 19, 256)       65536     \n                                                                 \n batch_normalization_2 (Bat  (None, 24, 19, 256)       1024      \n chNormalization)                                                \n                                                                 \n re_lu_2 (ReLU)              (None, 24, 19, 256)       0         \n                                                                 \n global_average_pooling2d (  (None, 256)               0         \n GlobalAveragePooling2D)                                         \n                                                                 \n dense (Dense)               (None, 8)                 2056      \n                                                                 \n softmax (Softmax)           (None, 8)                 0         \n                                                                 \n=================================================================\nTotal params: 143112 (559.03 KB)\nTrainable params: 141576 (553.03 KB)\nNon-trainable params: 1536 (6.00 KB)\n_________________________________________________________________\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/5f18d223-e66d-4237-b30c-e4b06dc4592a","content_dependencies":null},{"cell_type":"markdown","metadata":{"tags":[],"deepnote_app_coordinates":{"h":4,"w":12,"x":0,"y":169},"deepnote_app_block_visible":true,"cell_id":"55d60e393fa2422a855c4fa4977f5475","deepnote_cell_type":"markdown"},"source":"# Train the Model","block_group":"03a2a939d1a94cd2bdecefb37ccd95f1"},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693925847512,"execution_millis":1414946,"deepnote_app_coordinates":{"h":29,"w":12,"x":0,"y":174},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_output_hidden":true,"cell_id":"1d34059007394da29a0c95b3cbafda40","deepnote_cell_type":"code"},"source":"loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\ninitial_learning_rate = TRAINING_ARGS['initial_learning_rate']\nend_learning_rate = TRAINING_ARGS['end_learning_rate']\n\nlinear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=initial_learning_rate,\n    end_learning_rate=end_learning_rate,\n    decay_steps=len(train_ds) * epochs,\n)\noptimizer = tf.optimizers.Adam(learning_rate=linear_decay)\nmetrics = [tf.metrics.SparseCategoricalAccuracy()]\nmodel.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\nhistory = model.fit(train_ds, epochs=epochs, validation_data=val_ds)","block_group":"202b954c56114880aa0e4a60d6271497","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n320/320 [==============================] - 215s 667ms/step - loss: 2.0450 - sparse_categorical_accuracy: 0.1989 - val_loss: 1.9776 - val_sparse_categorical_accuracy: 0.2400\nEpoch 2/10\n320/320 [==============================] - 122s 383ms/step - loss: 1.5691 - sparse_categorical_accuracy: 0.4047 - val_loss: 1.4726 - val_sparse_categorical_accuracy: 0.4625\nEpoch 3/10\n320/320 [==============================] - 123s 385ms/step - loss: 1.0453 - sparse_categorical_accuracy: 0.6308 - val_loss: 0.9290 - val_sparse_categorical_accuracy: 0.6787\nEpoch 4/10\n320/320 [==============================] - 132s 412ms/step - loss: 0.7722 - sparse_categorical_accuracy: 0.7362 - val_loss: 0.9221 - val_sparse_categorical_accuracy: 0.6650\nEpoch 5/10\n320/320 [==============================] - 123s 383ms/step - loss: 0.6143 - sparse_categorical_accuracy: 0.7952 - val_loss: 0.6671 - val_sparse_categorical_accuracy: 0.7700\nEpoch 6/10\n320/320 [==============================] - 122s 381ms/step - loss: 0.5076 - sparse_categorical_accuracy: 0.8306 - val_loss: 0.5384 - val_sparse_categorical_accuracy: 0.8025\nEpoch 7/10\n320/320 [==============================] - 122s 380ms/step - loss: 0.4299 - sparse_categorical_accuracy: 0.8595 - val_loss: 0.4895 - val_sparse_categorical_accuracy: 0.8325\nEpoch 8/10\n320/320 [==============================] - 123s 383ms/step - loss: 0.3682 - sparse_categorical_accuracy: 0.8831 - val_loss: 0.4217 - val_sparse_categorical_accuracy: 0.8650\nEpoch 9/10\n320/320 [==============================] - 124s 387ms/step - loss: 0.3160 - sparse_categorical_accuracy: 0.9031 - val_loss: 0.3928 - val_sparse_categorical_accuracy: 0.8725\nEpoch 10/10\n320/320 [==============================] - 123s 383ms/step - loss: 0.2784 - sparse_categorical_accuracy: 0.9175 - val_loss: 0.3514 - val_sparse_categorical_accuracy: 0.9013\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/b3f71261-68e3-467f-810c-b4c3c7ec2652","content_dependencies":null},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693927262273,"execution_millis":255,"deepnote_app_coordinates":{"h":28,"w":12,"x":0,"y":204},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_output_hidden":true,"cell_id":"d7a050f90e2a418981b59e6316eadd91","deepnote_cell_type":"code"},"source":"history.history","block_group":"e37da420c84d4310b71af14f00dd80eb","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"{'loss': [2.0450072288513184,\n  1.5691094398498535,\n  1.0453083515167236,\n  0.7722204327583313,\n  0.6142938137054443,\n  0.5076009631156921,\n  0.4299235939979553,\n  0.3681630790233612,\n  0.31604674458503723,\n  0.27835968136787415],\n 'sparse_categorical_accuracy': [0.19890624284744263,\n  0.4046874940395355,\n  0.6307812333106995,\n  0.7362499833106995,\n  0.7951562404632568,\n  0.8306249976158142,\n  0.8595312237739563,\n  0.8831250071525574,\n  0.903124988079071,\n  0.9175000190734863],\n 'val_loss': [1.9776300191879272,\n  1.4726146459579468,\n  0.9289615750312805,\n  0.9221228957176208,\n  0.667147159576416,\n  0.5383950471878052,\n  0.48947229981422424,\n  0.4217441976070404,\n  0.3928365707397461,\n  0.3514421582221985],\n 'val_sparse_categorical_accuracy': [0.23999999463558197,\n  0.4625000059604645,\n  0.6787499785423279,\n  0.6650000214576721,\n  0.7699999809265137,\n  0.8025000095367432,\n  0.8324999809265137,\n  0.8650000095367432,\n  0.8725000023841858,\n  0.9012500047683716]}"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/1ad3f788-6edc-4251-a079-3661d4389469","content_dependencies":null},{"cell_type":"markdown","metadata":{"tags":[],"deepnote_app_coordinates":{"h":4,"w":12,"x":0,"y":233},"deepnote_app_block_visible":true,"cell_id":"ed8078fa8f2f4a9c87c2bdf20efd9dbd","deepnote_cell_type":"markdown"},"source":"# Test the model","block_group":"0cc77333017a4a10b766e22db4366186"},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693927262282,"execution_millis":20848,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":238},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_output_hidden":true,"cell_id":"02bb9d1b87c14026841853ff8e04796c","deepnote_cell_type":"code"},"source":"test_loss, test_accuracy = model.evaluate(test_ds)","block_group":"0b7291b7f9c64cf9bd369714a742b49f","execution_count":null,"outputs":[{"name":"stdout","text":"40/40 [==============================] - 14s 346ms/step - loss: 0.3192 - sparse_categorical_accuracy: 0.9038\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/1f5b4f0b-41d5-425a-bccf-20b21c26134b","content_dependencies":null},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693927283171,"execution_millis":19,"deepnote_app_coordinates":{"h":18,"w":12,"x":0,"y":244},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_output_hidden":true,"cell_id":"4b43e1cc9afc4674bc66e5f8bcf45738","deepnote_cell_type":"code"},"source":"training_loss = history.history['loss'][-1]\ntraining_accuracy = history.history['sparse_categorical_accuracy'][-1]\nval_loss = history.history['val_loss'][-1]\nval_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n\nprint(f'Training Loss: {training_loss:.4f}')\nprint(f'Training Accuracy: {training_accuracy*100.:.2f}%')\nprint()\nprint(f'Validation Loss: {val_loss:.4f}')\nprint(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\nprint()\nprint(f'Test Loss: {test_loss:.4f}')\nprint(f'Test Accuracy: {test_accuracy*100.:.2f}%')","block_group":"998ae4497b7f476392d6a64d4a929fd8","execution_count":null,"outputs":[{"name":"stdout","text":"Training Loss: 0.2784\nTraining Accuracy: 91.75%\n\nValidation Loss: 0.3514\nValidation Accuracy: 90.13%\n\nTest Loss: 0.3192\nTest Accuracy: 90.38%\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/f120bda7-7192-48d4-8c98-d4b2ed222537","content_dependencies":null},{"cell_type":"markdown","metadata":{"tags":[],"deepnote_app_coordinates":{"h":4,"w":12,"x":0,"y":263},"deepnote_app_block_visible":true,"cell_id":"3fbb760ff187440e867cba412d4223a6","deepnote_cell_type":"markdown"},"source":"# Save the Model","block_group":"12f3d65785d8497080c74cb75e7994d2"},{"cell_type":"markdown","metadata":{"tags":[],"deepnote_app_coordinates":{"h":3,"w":12,"x":0,"y":268},"deepnote_app_block_visible":true,"cell_id":"90939161bcc149ef930cf562fff71ed7","deepnote_cell_type":"markdown"},"source":"### Save Keras Model","block_group":"9075579638a148debbe63ecafa6042f4"},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693927703595,"execution_millis":1406,"deepnote_app_coordinates":{"h":11,"w":12,"x":0,"y":272},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_output_hidden":true,"cell_id":"f649c9cac2bd4e20be1840c05b3a09ff","deepnote_cell_type":"code"},"source":"import os\nfrom time import time\n\ntimestamp = int(time())\n\nsaved_model_dir = f'./saved_models/{timestamp}'\nif not os.path.exists(saved_model_dir):\n    os.makedirs(saved_model_dir)\nmodel.save(saved_model_dir)","block_group":"b7fb9ed099b541fda998172be56bc928","execution_count":null,"outputs":[{"name":"stdout","text":"INFO:tensorflow:Assets written to: ./saved_models/1693927703/assets\nINFO:tensorflow:Assets written to: ./saved_models/1693927703/assets\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/79b9e31d-4940-4077-a440-7d89a130a13e","content_dependencies":null},{"cell_type":"markdown","metadata":{"tags":[],"deepnote_app_coordinates":{"h":3,"w":12,"x":0,"y":284},"deepnote_app_block_visible":true,"cell_id":"54d5fe46d0944684b3aa6df072ef58c9","deepnote_cell_type":"markdown"},"source":"### Save Hyper-Parameters and Results","block_group":"365be160037c416381ddb33b866a933e"},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1693927707208,"execution_millis":10,"deepnote_app_coordinates":{"h":12,"w":12,"x":0,"y":288},"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_output_hidden":true,"cell_id":"0d82593ee2a74051bdfbfe100f01ab71","deepnote_cell_type":"code"},"source":"import pandas as pd\n\noutput_dict = {\n    'timestamp': timestamp,\n    **PREPROCESSING_ARGS,\n    **TRAINING_ARGS,\n    'test_accuracy': test_accuracy\n}\n\ndf = pd.DataFrame([output_dict])\n\noutput_path='./mel_spectrogram_dscnn_results.csv'\ndf.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)","block_group":"c65a64de61b24894bf8a1029ea2c75e7","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6f1fd91f-a434-4542-983d-3ce5ae14ac33' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2023-09-05T16:45:10.706Z"},"deepnote_app_comments_enabled":false,"deepnote_app_layout":"powerful-article","deepnote_app_table_of_contents_enabled":false,"deepnote_app_execution_enabled":false,"deepnote_app_run_on_input_enabled":false,"deepnote_notebook_id":"635ee8d59a6646a898cb06204ce79667","deepnote_execution_queue":[]}}